### K Means

1. **Ініціалізація кластерних центрів**

`n_samples = adj_matrix.shape[0]`
`centroids = adj_matrix[np.random.choice(n_samples, n_clusters, replace=False)]`

- `adj_matrix.shape[0]`: Визначає кількість рядків у матриці (кількість вузлів).
- `np.random.choice(n_samples, n_clusters, replace=False)`: Обирає `n_clusters` випадкових індексів без повторень з доступних вузлів.
    - **Приклад**: Якщо у вас 5 вузлів і ви хочете 2 кластери, функція вибере два випадкові вузли як початкові центроїди, наприклад, вузли 2 і 4.
- `centroids`: Із матриці `adj_matrix` беруться рядки (вузли), що відповідають обраним індексам. Це будуть початкові центроїди кластерів.

2. **Визначення кластера для кожної точки**

`distances = pairwise_distances(adj_matrix, centroids)`
`labels = np.argmin(distances, axis=1)`

`pairwise_distances`: Ця функція з бібліотеки `sklearn.metrics` обчислює відстані між кожним вузлом (рядком) та центроїдами кластерів.

- На вхід подаються два масиви: `adj_matrix` (вузли) і `centroids` (центроїди).

- **Результат**: Матриця розміром `(n_samples, n_clusters)`, де кожен елемент `[i, j]` — це відстань між вузлом `i` і центроїдом `j`.
- **Приклад**: Якщо у вас є 5 вузлів і 2 центроїди, результат виглядатиме так:
`[[1.2, 0.8],  # Відстань вузла 1 до центроїдів 1 і 2`
 `[0.5, 1.3],  # Відстань вузла 2 до центроїдів 1 і 2`
 `...]`
 
`np.argmin(distances, axis=1)`: Визначає індекс (номер) найближчого центроїда для кожного вузла.

- **Результат**: Одновимірний масив `labels`, де `labels[i]` — це кластер, до якого належить вузол `i`
`
3. **Перевірка на завершення**

`if np.array_equal(labels, prev_labels):`
    `break`

`np.array_equal(labels, prev_labels)`: Перевіряє, чи класифікація вузлів (`labels`) змінилася у порівнянні з попередньою ітерацією.

- Якщо класифікація не змінюється, алгоритм завершує роботу (знайдено стабільні кластери).

4. **Оновлення кластерних центрів**

`centroids = np.array([adj_matrix[labels == i].mean(axis=0) for i in range(n_clusters)])`

- Для кожного кластера (`i`) визначається новий центроїд:
    - `adj_matrix[labels == i]`: Вибирає всі вузли, які належать до кластера `i`.
    - `.mean(axis=0)`: Обчислює середнє значення цих вузлів по всіх координатах. Це новий центроїд.
- `centroids`: Містить оновлені центроїди для всіх кластерів.
    - **Приклад**: Якщо у кластері 1 вузли 2, 3 і 4, то новий центроїд — середнє цих вузлів.

5. **Формування результатів**

`clustered_nodes = {i: [] for i in range(n_clusters)}`

`for node, cluster in zip(nodes, labels):`
    `clustered_nodes[cluster].append(node)`


- `clustered_nodes`: Створює порожній словник, де ключі — це номери кластерів, а значення — списки вузлів, які до них належать.
- `zip(nodes, labels)`: Поєднує вузли і їхні класи (кластеризацію).
    - **Приклад**: Якщо `nodes = ['A', 'B', 'C']` і `labels = [0, 1, 0]`, то вузол `A` належить до кластера 0, вузол `B` — до кластера 1.
- `clustered_nodes[cluster].append(node)`: Додає вузол до відповідного кластера.


---
### Що роблять імпортовані функції?

1. **`numpy`**:
    
    - Використовується для роботи з масивами, обчислень середніх значень та випадкової ініціалізації.
    - Наприклад:
        - `np.random.choice` для вибору початкових центроїдів.
        - `np.array` для створення масивів.
2. **`pairwise_distances` (з `sklearn.metrics`)**:
    
    - Обчислює матрицю попарних відстаней між усіма вузлами і центроїдами.
    - Підтримує різні метрики, наприклад, "евклідову" відстань (за замовчуванням).

---

### Приклад роботи

- У вас є вузли `['A', 'B', 'C', 'D']` та матриця відстаней:
- `[[0.0, 1.2, 0.8, 2.5],`
 `[1.2, 0.0, 1.5, 0.9],`
 `[0.8, 1.5, 0.0, 1.1],`
 `[2.5, 0.9, 1.1, 0.0]]`

- Спочатку обираються випадкові центроїди, наприклад, вузли `A` та `C`.
- Алгоритм класифікує вузли за відстанню до центроїдів, наприклад:
    - Вузол `A` ближчий до центроїда 0.
    - Вузол `B` ближчий до центроїда 1.
- Алгоритм оновлює центроїди, обчислюючи середнє положення вузлів у кластері.
- Процес повторюється, поки кластери не стабілізуються.
---
### Додатково:

**Одновимірний масив** — це структура даних, що представляє собою лінійний список елементів.

Приклад одновимірного масиву:
`labels = [0, 1, 0, 2, 1]`

Тут:

- Міста з індексами 0 і 2 належать до кластеру 0.
- Міста з індексами 1 і 4 належать до кластеру 1.
- Місто з індексом 3 належить до кластеру 2.

У задачі, **одновимірний масив `labels`** зберігає мітки (класи) для кожного елемента після кластеризації, щоб вказати, до якого кластеру належить кожен елемент.

